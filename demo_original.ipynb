{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from argparse import Namespace\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.manual_seed(42)\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phoneme Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoneme_GAT.phoneme_model import BaseModule, load_phoneme_model, optim_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ay2.tools.text._phonemes import Phonemer_Tokenizer_Recombination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup Instructions:**\n",
    "\n",
    "1. The pretrained phoneme recognition model should be in the project root: `Best Epoch 42 Validation 0.407.ckpt`\n",
    "2. The vocab files should be in: `vocab_phoneme/` directory with all 9 language JSON files\n",
    "3. The paths are now automatically configured to use local files (no manual changes needed!)\n",
    "\n",
    "If the checkpoint is missing, download it from [Google Drive](https://drive.google.com/file/d/1SbqynkUQxxlhazklZz9OgcVK7Fl2aT-z/view?usp=drive_link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using checkpoint: /Users/arjunjindal/Desktop/PLFD-ADD/Best Epoch 42 Validation 0.407.ckpt\n",
      "✓ Checkpoint exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Use local checkpoint path (automatically finds it in project root)\n",
    "project_root = os.path.abspath(\".\")\n",
    "pretrained_path = os.path.join(project_root, \"Best Epoch 42 Validation 0.407.ckpt\")\n",
    "\n",
    "network_param = Namespace(\n",
    "    network_name=\"WavLM\",\n",
    "    pretrained_path=pretrained_path,\n",
    "    freeze=True,\n",
    "    freeze_transformer=True,\n",
    "    eos_token=\"</s>\",\n",
    "    bos_token=\"<s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    word_delimiter_token=\"|\",\n",
    "    vocab_size=200,\n",
    ")\n",
    "\n",
    "print(f\"✓ Using checkpoint: {pretrained_path}\")\n",
    "print(f\"✓ Checkpoint exists: {os.path.exists(pretrained_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the Phoneme Recognition Model:**\n",
    "\n",
    "The `load_phoneme_model` function now automatically:\n",
    "1. Uses the local checkpoint path specified above\n",
    "2. Finds vocab files in the `vocab_phoneme/` directory (relative path)\n",
    "3. Downloads the WavLM base model from HuggingFace if not cached\n",
    "\n",
    "✅ Everything is configured automatically - just run the cells!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, load vocab json files from  /Users/arjunjindal/Desktop/PLFD-ADD/vocab_phoneme Please make sure the vocab files are correct\n",
      "Load WavLM model!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WavLMForCTC were not initialized from the model checkpoint at microsoft/wavlm-base and are newly initialized: ['lm_head.weight', 'lm_head.bias', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([687, 768])\n"
     ]
    }
   ],
   "source": [
    "total_num_phonemes = 687  ## 198, or 687\n",
    "\n",
    "phoneme_model = load_phoneme_model(\n",
    "    network_name=network_param.network_name,\n",
    "    pretrained_path=network_param.pretrained_path,\n",
    "    total_num_phonemes=total_num_phonemes,\n",
    ")\n",
    "assert len(phoneme_model.tokenizer.total_phonemes) == total_num_phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoneme_GAT.modules import Phoneme_GAT_lit,Phoneme_GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, load vocab json files from  /Users/arjunjindal/Desktop/PLFD-ADD/vocab_phoneme Please make sure the vocab files are correct\n",
      "Load WavLM model!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WavLMForCTC were not initialized from the model checkpoint at microsoft/wavlm-base and are newly initialized: ['lm_head.weight', 'lm_head.bias', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([687, 768])\n"
     ]
    }
   ],
   "source": [
    "audio_model = Phoneme_GAT(\n",
    "    backbone='wavlm',\n",
    "    use_raw=0,\n",
    "    use_GAT=1,\n",
    "    n_edges=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random audio to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 1, 48000)\n",
    "num_frames = torch.full((x.shape[0],), 48000 // 320 - 1) # (batch_size,)\n",
    "res = audio_model(x, num_frames=num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit torch.Size([3])\n",
      "hidden_states torch.Size([3, 768])\n",
      "phoneme_feat torch.Size([3, 149, 768])\n",
      "encoder_feat torch.Size([3, 149, 768])\n",
      "phoneme_cls_logit torch.Size([3, 687])\n",
      "phoneme_cls_label torch.Size([3])\n",
      "aug_logit torch.Size([3])\n",
      "aug_frame_logit torch.Size([3])\n",
      "aug_labels torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for key, value in res.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The settings of the `AudioModel` are defined in the `cfg`. Each setting is a key-value pair, where the key is the name of the setting and the value is the value of the setting. The meaning of each setting is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Network Structure Parameters**:\n",
    "   - `backbone` : \"wavlm\", the backbone of the phoneme recognition model.\n",
    "   - `use_raw` : `False`, whether to use raw transformer as the backbone\n",
    "   - `use_GAT`: `True`, whether to use GAT\n",
    "   - `n_edges`: `10`, the nubmer of edges for each node in the GAT\n",
    "   - `use_pool`: `True`, whether to use pooling\n",
    "\n",
    "\n",
    "2. **Loss Function Parameters**:\n",
    "   - `use_clip`: `True`, whether to use clip loss\n",
    "\n",
    "\n",
    "3. **Data Augmentation and Training Strategy**:\n",
    "   - `use_aug`: `True`, whether to use data augmentation in the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "# Construct the configuration using Namespace\n",
    "cfg = Namespace(\n",
    "    PhonemeGAT=Namespace(\n",
    "        backbone=\"wavlm\",  # wavlm or wav2vec\n",
    "        use_raw=False,              # whether to use raw transformer as the backbone\n",
    "        use_GAT=True,              # whether to use GAT\n",
    "        n_edges=10,                # the nubmer of edges for each node in the GAT\n",
    "        use_aug=True,              # whether to use data augmentation in the training\n",
    "        use_pool=True,            # whether to use pooling\n",
    "        use_clip=True,             # whether to use clip loss\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.loggers import  CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the pytorch Lightning module to train the model, where we define the train step, validation/predict step, loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, load vocab json files from  /Users/arjunjindal/Desktop/PLFD-ADD/vocab_phoneme Please make sure the vocab files are correct\n",
      "Load WavLM model!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WavLMForCTC were not initialized from the model checkpoint at microsoft/wavlm-base and are newly initialized: ['lm_head.weight', 'lm_head.bias', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([687, 768])\n"
     ]
    }
   ],
   "source": [
    "audio_model_lit = Phoneme_GAT_lit(cfg=cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test forwarding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lit model, we use the `_shared_pred` method to predict the logits of the input batch. If the stage is train, we also the the audio_transform to augment the spectrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 1, 48000)\n",
    "batch = {\n",
    "    \"label\": torch.randint(0, 2, (3,)),\n",
    "    \"audio\": x,\n",
    "    \"sample_rate\": 16000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, you batch must be a dict with above keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit torch.Size([3])\n",
      "hidden_states torch.Size([3, 768])\n",
      "phoneme_feat torch.Size([3, 149, 768])\n",
      "encoder_feat torch.Size([3, 149, 768])\n",
      "phoneme_cls_logit torch.Size([3, 687])\n",
      "phoneme_cls_label torch.Size([3])\n",
      "aug_logit torch.Size([3])\n",
      "aug_frame_logit torch.Size([3])\n",
      "aug_labels torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "batch_res = audio_model_lit._shared_pred(batch=batch, batch_idx=0)\n",
    "for key, value in batch_res.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first build a simple dataloaders for training, where all the samples are randomly generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callbacks import EER_Callback, BinaryAUC_Callback, BinaryACC_Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTestDataset(Dataset):\n",
    "    def __init__(self, num_samples=10):\n",
    "        # Generate synthetic data similar to your example\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            self.samples.append({\n",
    "                \"audio\": torch.randn(1, 48000),\n",
    "                \"label\": torch.randint(0, 2, (1,)).item(),\n",
    "                \"sample_rate\": 16000,\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "test_dataset = SimpleTestDataset(num_samples=20)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a simple trainer to train and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using accelerator: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger path: ./logs/lightning_logs/version_5\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect GPU or CPU\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = \"gpu\"\n",
    "    devices = 1\n",
    "else:\n",
    "    accelerator = \"cpu\"\n",
    "    devices = \"auto\"\n",
    "\n",
    "print(f\"Using accelerator: {accelerator}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=CSVLogger(save_dir=\"./logs\", version=None),\n",
    "    max_epochs=4,\n",
    "    accelerator=accelerator,\n",
    "    devices=devices,\n",
    "    callbacks=[\n",
    "        BinaryACC_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "        BinaryAUC_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "        EER_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Logger path: {trainer.logger.log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type                    | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | model         | Phoneme_GAT             | 196 M  | train\n",
      "1 | bce_loss      | BCEWithLogitsLoss       | 0      | train\n",
      "2 | ce_loss       | CrossEntropyLoss        | 0      | train\n",
      "3 | contrast_loss | BinaryTokenContrastLoss | 0      | train\n",
      "4 | clip_head     | Sequential              | 1.2 M  | train\n",
      "5 | clip_loss     | CLIPLoss1D              | 1      | train\n",
      "------------------------------------------------------------------\n",
      "102 M     Trainable params\n",
      "94.9 M    Non-trainable params\n",
      "197 M     Total params\n",
      "790.544   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a746bbb9e8948ceb8080b01b3817401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(audio_model_lit, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, you can view the logging loss in the logger file, for example `logs/lightning_logs/version_0/metrics.csv`.\n",
    "![](imgs/loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing, the results will also saved in logger file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0ae0800a87407cbb521476e430961b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test-acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.44999998807907104    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test-auc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.59375          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test-aug_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test-clip_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     2.159363269805908     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test-cls_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.701637864112854     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test-eer          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.5            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test-loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.7813193798065186     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test-acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.44999998807907104   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test-auc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.59375         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test-aug_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test-clip_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.159363269805908    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test-cls_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.701637864112854    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test-eer         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.5           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test-loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.7813193798065186    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test-loss': 1.7813193798065186,\n",
       "  'test-cls_loss': 0.701637864112854,\n",
       "  'test-clip_loss': 2.159363269805908,\n",
       "  'test-aug_loss': 0.0,\n",
       "  'test-acc': 0.44999998807907104,\n",
       "  'test-auc': 0.59375,\n",
       "  'test-eer': 0.5}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(audio_model_lit, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
