{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from argparse import  Namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phoneme Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/anaconda3/envs/phoneme_deepfake_detection/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from phoneme_GAT.phoneme_model import BaseModule, load_phoneme_model, optim_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ay2.tools.text._phonemes import Phonemer_Tokenizer_Recombination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You can download the pretrained phoneme recognition model in [google drive](https://drive.google.com/file/d/1SbqynkUQxxlhazklZz9OgcVK7Fl2aT-z/view?usp=drive_link).\n",
    "2. Change `pretrained_path` to you own custom path.\n",
    "3. Remember to change the `pretrained_path` and `vocab_path` in the `load_phoneme_model` function of `phoneme_GAT.phoneme_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_param = Namespace(\n",
    "    network_name=\"WavLM\",\n",
    "    pretrained_path = \"/home/ay/data/phonemes/wavlm/best-epoch=42-val-per=0.407000.ckpt\",\n",
    "    freeze=True,\n",
    "    freeze_transformer=True,\n",
    "    eos_token=\"</s>\",\n",
    "    bos_token=\"<s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    word_delimiter_token=\"|\",\n",
    "    vocab_size=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the phoneme recognition model,\n",
    "1. you must specify the pretrained_path!!!! Please download the provided pretrained phoneme model; or you can train yourself model through `train_phoneme_model.py`.\n",
    "2. in the `load_phoneme_model` function, you have to change the correct `vocab_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, load vocab json files from  /home/ay/tmp/PLFD-ADD/vocab_phoneme Please make sure the vocab files are correct\n",
      "Load WavLM model!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WavLMForCTC were not initialized from the model checkpoint at /home/ay/.cache/huggingface/hub/models--microsoft--wavlm-base/snapshots/efa81aae7ff777e464159e0f877d54eac5b84f81/ and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([687, 768])\n"
     ]
    }
   ],
   "source": [
    "total_num_phonemes = 687  ## 198, or 687\n",
    "\n",
    "phoneme_model = load_phoneme_model(\n",
    "    network_name=network_param.network_name,\n",
    "    pretrained_path=network_param.pretrained_path,\n",
    "    total_num_phonemes=total_num_phonemes,\n",
    ")\n",
    "assert len(phoneme_model.tokenizer.total_phonemes) == total_num_phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoneme_GAT.modules import Phoneme_GAT_lit,Phoneme_GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, load vocab json files from  /home/ay/tmp/PLFD-ADD/vocab_phoneme Please make sure the vocab files are correct\n",
      "Load WavLM model!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WavLMForCTC were not initialized from the model checkpoint at /home/ay/.cache/huggingface/hub/models--microsoft--wavlm-base/snapshots/efa81aae7ff777e464159e0f877d54eac5b84f81/ and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([687, 768])\n"
     ]
    }
   ],
   "source": [
    "audio_model = Phoneme_GAT(\n",
    "    backbone='wavlm',\n",
    "    use_raw=0,\n",
    "    use_GAT=1,\n",
    "    n_edges=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random audio to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 1, 48000)\n",
    "num_frames = torch.full((x.shape[0],), 48000 // 320 - 1) # (batch_size,)\n",
    "res = audio_model(x, num_frames=num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit torch.Size([3])\n",
      "hidden_states torch.Size([3, 768])\n",
      "phoneme_feat torch.Size([3, 149, 768])\n",
      "encoder_feat torch.Size([3, 149, 768])\n",
      "phoneme_cls_logit torch.Size([3, 687])\n",
      "phoneme_cls_label torch.Size([3])\n",
      "aug_logit torch.Size([3])\n",
      "aug_frame_logit torch.Size([3])\n",
      "aug_labels torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for key, value in res.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The settings of the `AudioModel` are defined in the `cfg`. Each setting is a key-value pair, where the key is the name of the setting and the value is the value of the setting. The meaning of each setting is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Network Structure Parameters**:\n",
    "   - `backbone` : \"wavlm\", the backbone of the phoneme recognition model.\n",
    "   - `use_raw` : `False`, whether to use raw transformer as the backbone\n",
    "   - `use_GAT`: `True`, whether to use GAT\n",
    "   - `n_edges`: `10`, the nubmer of edges for each node in the GAT\n",
    "   - `use_pool`: `True`, whether to use pooling\n",
    "\n",
    "\n",
    "2. **Loss Function Parameters**:\n",
    "   - `use_clip`: `True`, whether to use clip loss\n",
    "\n",
    "\n",
    "3. **Data Augmentation and Training Strategy**:\n",
    "   - `use_aug`: `True`, whether to use data augmentation in the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "# Construct the configuration using Namespace\n",
    "cfg = Namespace(\n",
    "    PhonemeGAT=Namespace(\n",
    "        backbone=\"wavlm\",  # wavlm or wav2vec\n",
    "        use_raw=False,              # whether to use raw transformer as the backbone\n",
    "        use_GAT=True,              # whether to use GAT\n",
    "        n_edges=10,                # the nubmer of edges for each node in the GAT\n",
    "        use_aug=True,              # whether to use data augmentation in the training\n",
    "        use_pool=True,            # whether to use pooling\n",
    "        use_clip=True,             # whether to use clip loss\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.loggers import  CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the pytorch Lightning module to train the model, where we define the train step, validation/predict step, loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, load vocab json files from  /home/ay/tmp/PLFD-ADD/vocab_phoneme Please make sure the vocab files are correct\n",
      "Load WavLM model!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WavLMForCTC were not initialized from the model checkpoint at /home/ay/.cache/huggingface/hub/models--microsoft--wavlm-base/snapshots/efa81aae7ff777e464159e0f877d54eac5b84f81/ and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([687, 768])\n"
     ]
    }
   ],
   "source": [
    "audio_model_lit = Phoneme_GAT_lit(cfg=cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test forwarding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lit model, we use the `_shared_pred` method to predict the logits of the input batch. If the stage is train, we also the the audio_transform to augment the spectrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 1, 48000)\n",
    "batch = {\n",
    "    \"label\": torch.randint(0, 2, (3,)),\n",
    "    \"audio\": x,\n",
    "    \"sample_rate\": 16000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, you batch must be a dict with above keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit torch.Size([3])\n",
      "hidden_states torch.Size([3, 768])\n",
      "phoneme_feat torch.Size([3, 149, 768])\n",
      "encoder_feat torch.Size([3, 149, 768])\n",
      "phoneme_cls_logit torch.Size([3, 687])\n",
      "phoneme_cls_label torch.Size([3])\n",
      "aug_logit torch.Size([3])\n",
      "aug_frame_logit torch.Size([3])\n",
      "aug_labels torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "batch_res = audio_model_lit._shared_pred(batch=batch, batch_idx=0)\n",
    "for key, value in batch_res.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first build a simple dataloaders for training, where all the samples are randomly generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callbacks import EER_Callback, BinaryAUC_Callback, BinaryACC_Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTestDataset(Dataset):\n",
    "    def __init__(self, num_samples=10):\n",
    "        # Generate synthetic data similar to your example\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            self.samples.append({\n",
    "                \"audio\": torch.randn(1, 48000),\n",
    "                \"label\": torch.randint(0, 2, (1,)).item(),\n",
    "                \"sample_rate\": 16000,\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "test_dataset = SimpleTestDataset(num_samples=20)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a simple trainer to train and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    logger=CSVLogger(save_dir=\"./logs\", version=0),\n",
    "    max_epochs=4,\n",
    "    callbacks=[\n",
    "        BinaryACC_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "        BinaryAUC_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "        EER_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/anaconda3/envs/phoneme_deepfake_detection/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name          | Type                    | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | model         | Phoneme_GAT             | 196 M  | train\n",
      "1 | bce_loss      | BCEWithLogitsLoss       | 0      | train\n",
      "2 | ce_loss       | CrossEntropyLoss        | 0      | train\n",
      "3 | contrast_loss | BinaryTokenContrastLoss | 0      | train\n",
      "4 | clip_head     | Sequential              | 1.2 M  | train\n",
      "5 | clip_loss     | CLIPLoss1D              | 1      | train\n",
      "------------------------------------------------------------------\n",
      "102 M     Trainable params\n",
      "94.9 M    Non-trainable params\n",
      "197 M     Total params\n",
      "790.544   Total estimated model params size (MB)\n",
      "248       Modules in train mode\n",
      "239       Modules in eval mode\n",
      "/home/ay/anaconda3/envs/phoneme_deepfake_detection/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n",
      "/home/ay/anaconda3/envs/phoneme_deepfake_detection/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 7/7 [00:01<00:00,  4.11it/s, v_num=0, train-loss=2.250, train-cls_loss=0.666, train-clip_loss=2.880, train-aug_loss=0.295, train-acc=0.600, train-auc=0.495, train-eer=0.462]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 7/7 [00:07<00:00,  0.97it/s, v_num=0, train-loss=2.250, train-cls_loss=0.666, train-clip_loss=2.880, train-aug_loss=0.295, train-acc=0.600, train-auc=0.495, train-eer=0.462]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(audio_model_lit, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, you can view the logging loss in the logger file, for example `logs/lightning_logs/version_0/metrics.csv`.\n",
    "![](imgs/loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing, the results will also saved in logger file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/home/ay/anaconda3/envs/phoneme_deepfake_detection/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 7/7 [00:00<00:00,  8.32it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test-acc            0.6000000238418579\n",
      "        test-auc             0.791208803653717\n",
      "      test-aug_loss                 0.0\n",
      "     test-clip_loss          2.873018741607666\n",
      "      test-cls_loss         0.6275677680969238\n",
      "        test-eer            0.3076923191547394\n",
      "        test-loss            2.064077138900757\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test-loss': 2.064077138900757,\n",
       "  'test-cls_loss': 0.6275677680969238,\n",
       "  'test-clip_loss': 2.873018741607666,\n",
       "  'test-aug_loss': 0.0,\n",
       "  'test-acc': 0.6000000238418579,\n",
       "  'test-auc': 0.791208803653717,\n",
       "  'test-eer': 0.3076923191547394}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(audio_model_lit, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phoneme_deepfake_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
