{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504041f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from argparse import Namespace\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.manual_seed(42)\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PLFD Deepfake Detection - Demo Training\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3ea54f",
   "metadata": {},
   "source": [
    "# Phoneme-Level Deepfake Detection Training Demo\n",
    "\n",
    "This notebook demonstrates training the PLFD model for deepfake detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf571f8b",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**Setup Requirements:**\n",
    "1. Phoneme model checkpoint: `Best Epoch 42 Validation 0.407.ckpt` (in project root)\n",
    "2. Vocab files: `vocab_phoneme/` directory with 9 language JSON files\n",
    "3. HuggingFace token for dataset access (optional, for real data)\n",
    "\n",
    "**Choose Your Data Source:**\n",
    "- `USE_REAL_DATA = False` → Dummy random data (fast, for testing)\n",
    "- `USE_REAL_DATA = True` → ASVspoof 2019 LA dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Choose data source\n",
    "USE_REAL_DATA = False  # Set to True to use real ASVspoof data\n",
    "\n",
    "# HuggingFace token (optional, only needed for private datasets)\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "# Training settings\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 3\n",
    "NUM_TRAIN_SAMPLES = 20  # Small for demo\n",
    "\n",
    "print(f\"Data source: {'Real ASVspoof data' if USE_REAL_DATA else 'Dummy synthetic data'}\")\n",
    "print(f\"Training epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training samples: {NUM_TRAIN_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8933db2e",
   "metadata": {},
   "source": [
    "## Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ddcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect paths (works locally and on RunPod)\n",
    "project_root = os.path.abspath(\".\")\n",
    "pretrained_path = os.path.join(project_root, \"Best Epoch 42 Validation 0.407.ckpt\")\n",
    "vocab_path = os.path.join(project_root, \"vocab_phoneme\")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Checkpoint: {pretrained_path}\")\n",
    "print(f\"Checkpoint exists: {os.path.exists(pretrained_path)}\")\n",
    "print(f\"Vocab path: {vocab_path}\")\n",
    "print(f\"Vocab exists: {os.path.exists(vocab_path)}\")\n",
    "\n",
    "if not os.path.exists(pretrained_path):\n",
    "    print(\"\\n⚠️  ERROR: Checkpoint not found!\")\n",
    "    print(\"Download from: https://drive.google.com/file/d/1SbqynkUQxxlhazklZz9OgcVK7Fl2aT-z/view?usp=drive_link\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14027129",
   "metadata": {},
   "source": [
    "## Load Phoneme Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoneme_GAT.phoneme_model import BaseModule, load_phoneme_model, optim_param\n",
    "\n",
    "network_param = Namespace(\n",
    "    network_name=\"WavLM\",\n",
    "    pretrained_path=pretrained_path,\n",
    "    freeze=True,\n",
    "    freeze_transformer=True,\n",
    "    eos_token=\"</s>\",\n",
    "    bos_token=\"<s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    word_delimiter_token=\"|\",\n",
    "    vocab_size=200,\n",
    ")\n",
    "\n",
    "total_num_phonemes = 687  # 198 or 687\n",
    "\n",
    "print(\"Loading phoneme recognition model...\")\n",
    "phoneme_model = load_phoneme_model(\n",
    "    network_name=network_param.network_name,\n",
    "    pretrained_path=network_param.pretrained_path,\n",
    "    total_num_phonemes=total_num_phonemes,\n",
    ")\n",
    "\n",
    "assert len(phoneme_model.tokenizer.total_phonemes) == total_num_phonemes\n",
    "print(f\"✓ Phoneme model loaded ({total_num_phonemes} phonemes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db957d0",
   "metadata": {},
   "source": [
    "## Test Phoneme Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1c570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoneme_GAT.modules import Phoneme_GAT_lit, Phoneme_GAT\n",
    "\n",
    "print(\"Creating audio model...\")\n",
    "audio_model = Phoneme_GAT(\n",
    "    backbone='wavlm',\n",
    "    use_raw=0,\n",
    "    use_GAT=1,\n",
    "    n_edges=10,\n",
    ")\n",
    "\n",
    "# Test with random audio\n",
    "x = torch.randn(3, 1, 48000)\n",
    "num_frames = torch.full((x.shape[0],), 48000 // 320 - 1)\n",
    "res = audio_model(x, num_frames=num_frames)\n",
    "\n",
    "print(\"\\n✓ Audio model created successfully!\")\n",
    "print(\"\\nOutput shapes:\")\n",
    "for key, value in res.items():\n",
    "    print(f\"  {key:20s}: {str(value.shape):20s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a929bbb",
   "metadata": {},
   "source": [
    "## Create PyTorch Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03dbc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Namespace(\n",
    "    PhonemeGAT=Namespace(\n",
    "        backbone=\"wavlm\",\n",
    "        use_raw=False,\n",
    "        use_GAT=True,\n",
    "        n_edges=10,\n",
    "        use_aug=True,\n",
    "        use_pool=True,\n",
    "        use_clip=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Creating Lightning module...\")\n",
    "audio_model_lit = Phoneme_GAT_lit(cfg=cfg)\n",
    "\n",
    "# Test forward pass\n",
    "batch = {\n",
    "    \"label\": torch.randint(0, 2, (3,)),\n",
    "    \"audio\": torch.randn(3, 1, 48000),\n",
    "    \"sample_rate\": 16000,\n",
    "}\n",
    "\n",
    "batch_res = audio_model_lit._shared_pred(batch=batch, batch_idx=0, stage=\"train\")\n",
    "print(\"\\n✓ Lightning module working!\")\n",
    "print(\"\\nPrediction output shapes:\")\n",
    "for key, value in batch_res.items():\n",
    "    print(f\"  {key:20s}: {str(value.shape):20s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dbb382",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "\n",
    "This cell creates either:\n",
    "- **Dummy data**: Random synthetic audio for quick testing\n",
    "- **Real data**: ASVspoof 2019 LA dataset from HuggingFace (requires download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5536e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "if USE_REAL_DATA:\n",
    "    print(\"Loading real ASVspoof 2019 LA dataset from HuggingFace...\")\n",
    "    print(\"This may take a few minutes on first run (downloads ~1.6GB)\")\n",
    "    \n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        import torchaudio\n",
    "        import numpy as np\n",
    "        \n",
    "        # Login to HuggingFace if token provided\n",
    "        if HF_TOKEN:\n",
    "            from huggingface_hub import login\n",
    "            login(token=HF_TOKEN, add_to_git_credential=True)\n",
    "        \n",
    "        # Load dataset\n",
    "        dataset = load_dataset(\"Bisher/ASVspoof_2019_LA\")\n",
    "        train_data = dataset['train'].select(range(min(NUM_TRAIN_SAMPLES, len(dataset['train']))))\n",
    "        \n",
    "        class RealDataset(Dataset):\n",
    "            def __init__(self, hf_dataset):\n",
    "                self.dataset = hf_dataset\n",
    "            \n",
    "            def __len__(self):\n",
    "                return len(self.dataset)\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                item = self.dataset[idx]\n",
    "                \n",
    "                # Get audio\n",
    "                if 'audio' in item and isinstance(item['audio'], dict):\n",
    "                    if 'array' in item['audio']:\n",
    "                        audio = torch.tensor(item['audio']['array'], dtype=torch.float32)\n",
    "                        if audio.ndim == 1:\n",
    "                            audio = audio.unsqueeze(0)\n",
    "                    else:\n",
    "                        audio = torch.randn(1, 48000)\n",
    "                else:\n",
    "                    audio = torch.randn(1, 48000)\n",
    "                \n",
    "                # Pad/trim to 48000\n",
    "                if audio.shape[1] < 48000:\n",
    "                    audio = torch.nn.functional.pad(audio, (0, 48000 - audio.shape[1]))\n",
    "                elif audio.shape[1] > 48000:\n",
    "                    audio = audio[:, :48000]\n",
    "                \n",
    "                # Get label\n",
    "                label = 0 if item['label'] == 'bonafide' else 1\n",
    "                \n",
    "                return {\n",
    "                    \"audio\": audio,\n",
    "                    \"label\": label,\n",
    "                    \"sample_rate\": 16000,\n",
    "                }\n",
    "        \n",
    "        test_dataset = RealDataset(train_data)\n",
    "        print(f\"✓ Loaded {len(test_dataset)} real audio samples\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error loading real data: {e}\")\n",
    "        print(\"Falling back to dummy data...\")\n",
    "        USE_REAL_DATA = False\n",
    "\n",
    "if not USE_REAL_DATA:\n",
    "    print(\"Using dummy synthetic data for quick testing...\")\n",
    "    \n",
    "    class DummyDataset(Dataset):\n",
    "        def __init__(self, num_samples=20):\n",
    "            self.samples = []\n",
    "            for _ in range(num_samples):\n",
    "                self.samples.append({\n",
    "                    \"audio\": torch.randn(1, 48000),\n",
    "                    \"label\": torch.randint(0, 2, (1,)).item(),\n",
    "                    \"sample_rate\": 16000,\n",
    "                })\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.samples)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return self.samples[idx]\n",
    "    \n",
    "    test_dataset = DummyDataset(num_samples=NUM_TRAIN_SAMPLES)\n",
    "    print(f\"✓ Created {len(test_dataset)} dummy samples\")\n",
    "\n",
    "# Create dataloader\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # 0 for compatibility\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ DataLoader ready: {len(test_dataloader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2469be5",
   "metadata": {},
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6802f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from callbacks import EER_Callback, BinaryAUC_Callback, BinaryACC_Callback\n",
    "\n",
    "# Auto-detect GPU or CPU\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = \"gpu\"\n",
    "    devices = 1\n",
    "    print(\"✓ Using GPU acceleration\")\n",
    "else:\n",
    "    accelerator = \"cpu\"\n",
    "    devices = \"auto\"\n",
    "    print(\"✓ Using CPU\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=CSVLogger(save_dir=\"./logs\", version=None),\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    accelerator=accelerator,\n",
    "    devices=devices,\n",
    "    callbacks=[\n",
    "        BinaryACC_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "        BinaryAUC_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "        EER_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "    ],\n",
    "    enable_progress_bar=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Accelerator: {accelerator}\")\n",
    "print(f\"  Max epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Log directory: {trainer.logger.log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58edce95",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc341a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "trainer.fit(audio_model_lit, test_dataloader)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Training completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782879cc",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Testing model...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = trainer.test(audio_model_lit, test_dataloader)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ DEMO COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nResults saved to: {trainer.logger.log_dir}\")\n",
    "print(f\"Metrics CSV: {trainer.logger.log_dir}/metrics.csv\")\n",
    "print(\"\\nTest Results:\")\n",
    "for key, value in results[0].items():\n",
    "    print(f\"  {key:20s}: {value:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dd1057",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✅ Loading the pretrained phoneme recognition model\n",
    "2. ✅ Creating the Phoneme_GAT deepfake detection model\n",
    "3. ✅ Setting up PyTorch Lightning training\n",
    "4. ✅ Training on dummy or real ASVspoof data\n",
    "5. ✅ Evaluating the model\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**For RunPod deployment:**\n",
    "1. Upload this notebook and all code to RunPod\n",
    "2. Run `bash setup_runpod.sh` to install dependencies\n",
    "3. Set `USE_REAL_DATA = True` to use full dataset\n",
    "4. Increase `NUM_EPOCHS` and `NUM_TRAIN_SAMPLES` for production training\n",
    "\n",
    "**For local use:**\n",
    "- Metrics are saved in the logs directory\n",
    "- View training progress: `cat logs/lightning_logs/version_X/metrics.csv`\n",
    "- Best checkpoint is saved automatically\n",
    "\n",
    "**Configuration for full training:**\n",
    "```python\n",
    "USE_REAL_DATA = True\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 16  # Adjust based on GPU memory\n",
    "NUM_TRAIN_SAMPLES = -1  # Use full dataset\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
