{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "504041f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PLFD Deepfake Detection - Demo Training\n",
      "================================================================================\n",
      "PyTorch version: 2.5.1\n",
      "CUDA available: False\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from argparse import Namespace\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.manual_seed(42)\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PLFD Deepfake Detection - Demo Training\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3ea54f",
   "metadata": {},
   "source": [
    "# Phoneme-Level Deepfake Detection Training Demo\n",
    "\n",
    "This notebook demonstrates training the PLFD model for deepfake detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf571f8b",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**Setup Requirements:**\n",
    "1. Phoneme model checkpoint: `Best Epoch 42 Validation 0.407.ckpt` (in project root)\n",
    "2. Vocab files: `vocab_phoneme/` directory with 9 language JSON files\n",
    "3. HuggingFace token for dataset access (recommended)\n",
    "\n",
    "**Dataset:**\n",
    "This notebook uses the ASVspoof 2019 LA dataset from HuggingFace.\n",
    "- Will download automatically (~1.6GB on first run)\n",
    "- Create a `.env` file with your HuggingFace token: `HF_TOKEN=your_token_here`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ee23fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source: Real ASVspoof 2019 LA dataset\n",
      "Training epochs: 4\n",
      "Batch size: 3\n",
      "Training samples: 20\n",
      "HF Token loaded: ✅ Yes\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# HuggingFace token (load from .env file for security)\n",
    "# Create a .env file with: HF_TOKEN=your_token_here\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"\")  # Will be empty if .env not found\n",
    "\n",
    "# Training settings\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 3\n",
    "NUM_TRAIN_SAMPLES = 20  # Small for demo\n",
    "\n",
    "print(f\"Data source: Real ASVspoof 2019 LA dataset\")\n",
    "print(f\"Training epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training samples: {NUM_TRAIN_SAMPLES}\")\n",
    "print(f\"HF Token loaded: {'✅ Yes' if HF_TOKEN else '❌ No (.env file not found)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8933db2e",
   "metadata": {},
   "source": [
    "## Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b78ddcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/arjunjindal/Desktop/PLFD-ADD\n",
      "Checkpoint: /Users/arjunjindal/Desktop/PLFD-ADD/Best Epoch 42 Validation 0.407.ckpt\n",
      "Checkpoint exists: True\n",
      "Vocab path: /Users/arjunjindal/Desktop/PLFD-ADD/vocab_phoneme\n",
      "Vocab exists: True\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect paths (works locally and on RunPod)\n",
    "project_root = os.path.abspath(\".\")\n",
    "pretrained_path = os.path.join(project_root, \"Best Epoch 42 Validation 0.407.ckpt\")\n",
    "vocab_path = os.path.join(project_root, \"vocab_phoneme\")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Checkpoint: {pretrained_path}\")\n",
    "print(f\"Checkpoint exists: {os.path.exists(pretrained_path)}\")\n",
    "print(f\"Vocab path: {vocab_path}\")\n",
    "print(f\"Vocab exists: {os.path.exists(vocab_path)}\")\n",
    "\n",
    "if not os.path.exists(pretrained_path):\n",
    "    print(\"\\n⚠️  ERROR: Checkpoint not found!\")\n",
    "    print(\"Download from: https://drive.google.com/file/d/1SbqynkUQxxlhazklZz9OgcVK7Fl2aT-z/view?usp=drive_link\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14027129",
   "metadata": {},
   "source": [
    "## Load Phoneme Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "26ef779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading phoneme recognition model...\n",
      "Now, load vocab json files from  /Users/arjunjindal/Desktop/PLFD-ADD/vocab_phoneme Please make sure the vocab files are correct\n",
      "Load WavLM model!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WavLMForCTC were not initialized from the model checkpoint at microsoft/wavlm-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([687, 768])\n",
      "✓ Phoneme model loaded (687 phonemes)\n"
     ]
    }
   ],
   "source": [
    "from phoneme_GAT.phoneme_model import BaseModule, load_phoneme_model, optim_param\n",
    "\n",
    "network_param = Namespace(\n",
    "    network_name=\"WavLM\",\n",
    "    pretrained_path=pretrained_path,\n",
    "    freeze=True,\n",
    "    freeze_transformer=True,\n",
    "    eos_token=\"</s>\",\n",
    "    bos_token=\"<s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    word_delimiter_token=\"|\",\n",
    "    vocab_size=200,\n",
    ")\n",
    "\n",
    "total_num_phonemes = 687  # 198 or 687\n",
    "\n",
    "print(\"Loading phoneme recognition model...\")\n",
    "phoneme_model = load_phoneme_model(\n",
    "    network_name=network_param.network_name,\n",
    "    pretrained_path=network_param.pretrained_path,\n",
    "    total_num_phonemes=total_num_phonemes,\n",
    ")\n",
    "\n",
    "assert len(phoneme_model.tokenizer.total_phonemes) == total_num_phonemes\n",
    "print(f\"✓ Phoneme model loaded ({total_num_phonemes} phonemes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db957d0",
   "metadata": {},
   "source": [
    "## Test Phoneme Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "df1c570e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating audio model...\n",
      "Now, load vocab json files from  /Users/arjunjindal/Desktop/PLFD-ADD/vocab_phoneme Please make sure the vocab files are correct\n",
      "Load WavLM model!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WavLMForCTC were not initialized from the model checkpoint at microsoft/wavlm-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([687, 768])\n",
      "\n",
      "✓ Audio model created successfully!\n",
      "\n",
      "Output shapes:\n",
      "  logit               : torch.Size([3])     \n",
      "  hidden_states       : torch.Size([3, 768])\n",
      "  phoneme_feat        : torch.Size([3, 149, 768])\n",
      "  encoder_feat        : torch.Size([3, 149, 768])\n",
      "  phoneme_cls_logit   : torch.Size([3, 687])\n",
      "  phoneme_cls_label   : torch.Size([3])     \n",
      "  aug_logit           : torch.Size([3])     \n",
      "  aug_frame_logit     : torch.Size([3])     \n",
      "  aug_labels          : torch.Size([3])     \n"
     ]
    }
   ],
   "source": [
    "from phoneme_GAT.modules import Phoneme_GAT_lit, Phoneme_GAT\n",
    "\n",
    "print(\"Creating audio model...\")\n",
    "audio_model = Phoneme_GAT(\n",
    "    backbone='wavlm',\n",
    "    use_raw=0,\n",
    "    use_GAT=1,\n",
    "    n_edges=10,\n",
    ")\n",
    "\n",
    "# Test with random audio\n",
    "x = torch.randn(3, 1, 48000)\n",
    "num_frames = torch.full((x.shape[0],), 48000 // 320 - 1)\n",
    "res = audio_model(x, num_frames=num_frames)\n",
    "\n",
    "print(\"\\n✓ Audio model created successfully!\")\n",
    "print(\"\\nOutput shapes:\")\n",
    "for key, value in res.items():\n",
    "    print(f\"  {key:20s}: {str(value.shape):20s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a929bbb",
   "metadata": {},
   "source": [
    "## Create PyTorch Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e03dbc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Lightning module...\n",
      "Now, load vocab json files from  /Users/arjunjindal/Desktop/PLFD-ADD/vocab_phoneme Please make sure the vocab files are correct\n",
      "Load WavLM model!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WavLMForCTC were not initialized from the model checkpoint at microsoft/wavlm-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([687, 768])\n",
      "\n",
      "✓ Lightning module working!\n",
      "\n",
      "Prediction output shapes:\n",
      "  logit               : torch.Size([3])     \n",
      "  hidden_states       : torch.Size([3, 768])\n",
      "  phoneme_feat        : torch.Size([3, 149, 768])\n",
      "  encoder_feat        : torch.Size([3, 149, 768])\n",
      "  phoneme_cls_logit   : torch.Size([3, 687])\n",
      "  phoneme_cls_label   : torch.Size([3])     \n",
      "  aug_logit           : torch.Size([3])     \n",
      "  aug_frame_logit     : torch.Size([3])     \n",
      "  aug_labels          : torch.Size([3])     \n"
     ]
    }
   ],
   "source": [
    "cfg = Namespace(\n",
    "    PhonemeGAT=Namespace(\n",
    "        backbone=\"wavlm\",\n",
    "        use_raw=False,\n",
    "        use_GAT=True,\n",
    "        n_edges=10,\n",
    "        use_aug=True,\n",
    "        use_pool=True,\n",
    "        use_clip=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Creating Lightning module...\")\n",
    "audio_model_lit = Phoneme_GAT_lit(cfg=cfg)\n",
    "\n",
    "# Test forward pass\n",
    "batch = {\n",
    "    \"label\": torch.randint(0, 2, (3,)),\n",
    "    \"audio\": torch.randn(3, 1, 48000),\n",
    "    \"sample_rate\": 16000,\n",
    "}\n",
    "\n",
    "batch_res = audio_model_lit._shared_pred(batch=batch, batch_idx=0, stage=\"train\")\n",
    "print(\"\\n✓ Lightning module working!\")\n",
    "print(\"\\nPrediction output shapes:\")\n",
    "for key, value in batch_res.items():\n",
    "    print(f\"  {key:20s}: {str(value.shape):20s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dbb382",
   "metadata": {},
   "source": [
    "## Load ASVspoof 2019 LA Dataset\n",
    "\n",
    "This cell loads the real ASVspoof 2019 LA dataset from HuggingFace.\n",
    "- Dataset will be downloaded on first run (~1.6GB)\n",
    "- Requires HuggingFace token for access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5536e50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Loading ASVspoof 2019 LA dataset from HuggingFace...\n",
      "================================================================================\n",
      "\n",
      "Locating downloaded parquet files...\n",
      "✓ Found cache directory: /Users/arjunjindal/.cache/huggingface/hub/datasets--Bisher--ASVspoof_2019_LA/snapshots/aea92dd83a9c56e070c0b1e9f02e7c0d96216a4c/data\n",
      "\n",
      "Loading from parquet files using pandas...\n",
      "  Resolved parquet file: b4eea1063bbcfa0c1cef1b69a96ad8b787c32f662005562b899cd4b461739619\n",
      "  Reading parquet file...\n",
      "✓ Loaded 25380 samples from parquet\n",
      "✓ Selected 20 samples for training\n",
      "✓ Converted to HuggingFace dataset\n",
      "\n",
      "Inspecting first sample...\n",
      "  Keys: ['speaker_id', 'audio_file_name', 'audio', 'system_id', 'key']\n",
      "  Found label column: 'key' = 0\n",
      "  Audio type: <class 'dict'>\n",
      "  Audio dict keys: ['bytes', 'path']\n",
      "    'bytes': type=<class 'bytes'>, value=b'fLaC\\x00\\x00\\x00\"\\x04\\x80\\x04\\x80\\x00\\x00\\x1a\\x00\\x07\\xe2\\x03\\xe8\\x00\\xf0\\x00\\x00\\xd8!\\x91\\x97\\xff...\n",
      "    'path': type=<class 'str'>, value=LA_T_1138215.flac...\n",
      "\n",
      "✓ ASVspoofDataset initialized with 20 samples\n",
      "  Using label column: 'key'\n",
      "\n",
      "Testing dataset __getitem__...\n",
      "  Audio shape: torch.Size([1, 48000])\n",
      "  Label: 0\n",
      "  Sample rate: 16000\n",
      "\n",
      "✓ DataLoader created: 7 batches\n",
      "\n",
      "Testing batch loading...\n",
      "  Batch audio shape: torch.Size([3, 1, 48000])\n",
      "  Batch labels shape: torch.Size([3])\n",
      "  Batch labels: tensor([0, 0, 0])\n",
      "\n",
      "================================================================================\n",
      "✓ Dataset loading complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "import torchaudio\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Loading ASVspoof 2019 LA dataset from HuggingFace...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find the parquet files in your existing cache\n",
    "print(\"\\nLocating downloaded parquet files...\")\n",
    "hub_cache = Path.home() / \".cache\" / \"huggingface\" / \"hub\" / \"datasets--Bisher--ASVspoof_2019_LA\"\n",
    "snapshot_dir = hub_cache / \"snapshots\" / \"aea92dd83a9c56e070c0b1e9f02e7c0d96216a4c\" / \"data\"\n",
    "\n",
    "if not snapshot_dir.exists():\n",
    "    print(\"❌ Cached files not found. Please download the dataset first.\")\n",
    "    raise FileNotFoundError(f\"Cache directory not found: {snapshot_dir}\")\n",
    "\n",
    "print(f\"✓ Found cache directory: {snapshot_dir}\")\n",
    "\n",
    "# Load directly from parquet files using pandas (bypasses all cache issues)\n",
    "print(\"\\nLoading from parquet files using pandas...\")\n",
    "train_parquet_link = snapshot_dir / \"train-00000-of-00001.parquet\"\n",
    "\n",
    "# Resolve the symlink to get the actual blob file\n",
    "train_parquet = train_parquet_link.resolve()\n",
    "print(f\"  Resolved parquet file: {train_parquet.name}\")\n",
    "\n",
    "# Load with pandas (more reliable)\n",
    "import pandas as pd\n",
    "print(\"  Reading parquet file...\")\n",
    "df = pd.read_parquet(train_parquet)\n",
    "print(f\"✓ Loaded {len(df)} samples from parquet\")\n",
    "\n",
    "# Select subset for demo\n",
    "df_subset = df.head(NUM_TRAIN_SAMPLES)\n",
    "print(f\"✓ Selected {len(df_subset)} samples for training\")\n",
    "\n",
    "# Convert to HuggingFace dataset\n",
    "train_data = HFDataset.from_pandas(df_subset, preserve_index=False)\n",
    "print(f\"✓ Converted to HuggingFace dataset\")\n",
    "\n",
    "# Inspect first sample\n",
    "print(\"\\nInspecting first sample...\")\n",
    "first_sample = train_data[0]\n",
    "print(f\"  Keys: {list(first_sample.keys())}\")\n",
    "\n",
    "# Check what label column is called\n",
    "label_key = None\n",
    "for key in first_sample.keys():\n",
    "    if 'label' in key.lower() or 'key' in key.lower():\n",
    "        label_key = key\n",
    "        print(f\"  Found label column: '{label_key}' = {first_sample[label_key]}\")\n",
    "        break\n",
    "\n",
    "if 'audio' in first_sample:\n",
    "    audio_data = first_sample['audio']\n",
    "    print(f\"  Audio type: {type(audio_data)}\")\n",
    "    if isinstance(audio_data, dict):\n",
    "        print(f\"  Audio dict keys: {list(audio_data.keys())}\")\n",
    "        for key in audio_data.keys():\n",
    "            val = audio_data[key]\n",
    "            print(f\"    '{key}': type={type(val)}, value={str(val)[:100]}...\")\n",
    "    else:\n",
    "        print(f\"  Audio value type: {type(audio_data)}\")\n",
    "        if hasattr(audio_data, 'shape'):\n",
    "            print(f\"  Audio shape: {audio_data.shape}\")\n",
    "        else:\n",
    "            print(f\"  Audio value: {str(audio_data)[:200]}...\")\n",
    "\n",
    "class ASVspoofDataset(Dataset):\n",
    "    \"\"\"Dataset wrapper for ASVspoof 2019 LA\"\"\"\n",
    "    \n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "        \n",
    "        # Auto-detect label column name\n",
    "        sample_keys = list(hf_dataset[0].keys())\n",
    "        self.label_key = None\n",
    "        for key in sample_keys:\n",
    "            if 'label' in key.lower() or 'key' in key.lower():\n",
    "                self.label_key = key\n",
    "                break\n",
    "        \n",
    "        if self.label_key is None:\n",
    "            print(f\"  Warning: No label column found. Available columns: {sample_keys}\")\n",
    "            print(f\"  Using first column as label: '{sample_keys[0]}'\")\n",
    "            self.label_key = sample_keys[0]\n",
    "        \n",
    "        print(f\"\\n✓ ASVspoofDataset initialized with {len(self.dataset)} samples\")\n",
    "        print(f\"  Using label column: '{self.label_key}'\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Extract audio array - handle multiple possible formats\n",
    "        audio_array = None\n",
    "        sample_rate = 16000\n",
    "        \n",
    "        if 'audio' in item:\n",
    "            audio_data = item['audio']\n",
    "            \n",
    "            if isinstance(audio_data, dict):\n",
    "                # Audio is a dict - check for different possible keys\n",
    "                if 'array' in audio_data:\n",
    "                    audio_array = audio_data['array']\n",
    "                    sample_rate = audio_data.get('sampling_rate', 16000)\n",
    "                elif 'bytes' in audio_data:\n",
    "                    # Encoded audio bytes - need to decode\n",
    "                    import io\n",
    "                    import soundfile as sf\n",
    "                    audio_bytes = audio_data['bytes']\n",
    "                    audio_array, sample_rate = sf.read(io.BytesIO(audio_bytes))\n",
    "                elif 'path' in audio_data:\n",
    "                    # File path - need to load\n",
    "                    import soundfile as sf\n",
    "                    audio_array, sample_rate = sf.read(audio_data['path'])\n",
    "                else:\n",
    "                    raise ValueError(f\"Sample {idx}: Audio dict has unexpected keys: {list(audio_data.keys())}\")\n",
    "            \n",
    "            elif hasattr(audio_data, 'shape'):\n",
    "                # Audio is directly a numpy array\n",
    "                audio_array = audio_data\n",
    "            \n",
    "            elif isinstance(audio_data, (list, tuple)):\n",
    "                # Audio is a list/tuple\n",
    "                import numpy as np\n",
    "                audio_array = np.array(audio_data)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"Sample {idx}: Unexpected audio type: {type(audio_data)}\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Sample {idx}: No 'audio' key in item. Keys: {list(item.keys())}\")\n",
    "        \n",
    "        if audio_array is None:\n",
    "            raise ValueError(f\"Sample {idx}: Could not extract audio array\")\n",
    "        \n",
    "        # Convert to torch tensor and ensure 2D [channels, samples]\n",
    "        audio = torch.tensor(audio_array, dtype=torch.float32)\n",
    "        if audio.ndim == 1:\n",
    "            audio = audio.unsqueeze(0)  # Add channel dimension\n",
    "        elif audio.ndim > 2:\n",
    "            raise ValueError(f\"Sample {idx}: Unexpected audio dimensions: {audio.shape}\")\n",
    "        \n",
    "        # Resample if needed (ASVspoof is 16kHz, keep it at 16kHz)\n",
    "        # Model expects 48000 samples at 16kHz = 3 seconds\n",
    "        target_length = 48000\n",
    "        \n",
    "        # Pad or trim to target length\n",
    "        if audio.shape[1] < target_length:\n",
    "            # Pad with zeros\n",
    "            audio = torch.nn.functional.pad(audio, (0, target_length - audio.shape[1]))\n",
    "        elif audio.shape[1] > target_length:\n",
    "            # Trim to target length\n",
    "            audio = audio[:, :target_length]\n",
    "        \n",
    "        # Get label - handle different possible values\n",
    "        label_value = item[self.label_key]\n",
    "        if isinstance(label_value, str):\n",
    "            # String label: bonafide=0, spoof/fake=1\n",
    "            label = 0 if 'bonafide' in label_value.lower() else 1\n",
    "        else:\n",
    "            # Already numeric\n",
    "            label = int(label_value)\n",
    "        \n",
    "        return {\n",
    "            \"audio\": audio,\n",
    "            \"label\": label,\n",
    "            \"sample_rate\": sample_rate,\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "test_dataset = ASVspoofDataset(train_data)\n",
    "\n",
    "# Test first item\n",
    "print(\"\\nTesting dataset __getitem__...\")\n",
    "sample = test_dataset[0]\n",
    "print(f\"  Audio shape: {sample['audio'].shape}\")\n",
    "print(f\"  Label: {sample['label']}\")\n",
    "print(f\"  Sample rate: {sample['sample_rate']}\")\n",
    "\n",
    "# Create dataloader\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ DataLoader created: {len(test_dataloader)} batches\")\n",
    "\n",
    "# Test a batch\n",
    "print(\"\\nTesting batch loading...\")\n",
    "test_batch = next(iter(test_dataloader))\n",
    "print(f\"  Batch audio shape: {test_batch['audio'].shape}\")\n",
    "print(f\"  Batch labels shape: {test_batch['label'].shape}\")\n",
    "print(f\"  Batch labels: {test_batch['label']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Dataset loading complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2469be5",
   "metadata": {},
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cf6802f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using CPU\n",
      "\n",
      "Training configuration:\n",
      "  Accelerator: cpu\n",
      "  Max epochs: 4\n",
      "  Log directory: ./logs/lightning_logs/version_11\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from callbacks import EER_Callback, BinaryAUC_Callback, BinaryACC_Callback\n",
    "\n",
    "# Auto-detect GPU or CPU\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = \"gpu\"\n",
    "    devices = 1\n",
    "    print(\"✓ Using GPU acceleration\")\n",
    "else:\n",
    "    accelerator = \"cpu\"\n",
    "    devices = \"auto\"\n",
    "    print(\"✓ Using CPU\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=CSVLogger(save_dir=\"./logs\", version=None),\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    accelerator=accelerator,\n",
    "    devices=devices,\n",
    "    callbacks=[\n",
    "        BinaryACC_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "        BinaryAUC_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "        EER_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "    ],\n",
    "    enable_progress_bar=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Accelerator: {accelerator}\")\n",
    "print(f\"  Max epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Log directory: {trainer.logger.log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58edce95",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0bc341a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type                    | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | model         | Phoneme_GAT             | 196 M  | train\n",
      "1 | bce_loss      | BCEWithLogitsLoss       | 0      | train\n",
      "2 | ce_loss       | CrossEntropyLoss        | 0      | train\n",
      "3 | contrast_loss | BinaryTokenContrastLoss | 0      | train\n",
      "4 | clip_head     | Sequential              | 1.2 M  | train\n",
      "5 | clip_loss     | CLIPLoss1D              | 1      | train\n",
      "------------------------------------------------------------------\n",
      "102 M     Trainable params\n",
      "94.9 M    Non-trainable params\n",
      "197 M     Total params\n",
      "790.544   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting training...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe1e7322dd44382b76f65e698d03f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✓ Training completed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "trainer.fit(audio_model_lit, test_dataloader)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Training completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782879cc",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f685b0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Testing model...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f8cb7843964e7882da0e8ea744b98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test-acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            1.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test-auc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test-aug_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test-clip_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6701117157936096     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test-cls_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4631733000278473     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test-eer          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            1.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test-loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7982291579246521     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test-acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test-auc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test-aug_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test-clip_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6701117157936096    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test-cls_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4631733000278473    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test-eer         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test-loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7982291579246521    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✓ DEMO COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "Results saved to: ./logs/lightning_logs/version_11\n",
      "Metrics CSV: ./logs/lightning_logs/version_11/metrics.csv\n",
      "\n",
      "Test Results:\n",
      "  test-loss           : 0.7982\n",
      "  test-cls_loss       : 0.4632\n",
      "  test-clip_loss      : 0.6701\n",
      "  test-aug_loss       : 0.0000\n",
      "  test-acc            : 1.0000\n",
      "  test-auc            : 0.0000\n",
      "  test-eer            : 1.0000\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Testing model...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = trainer.test(audio_model_lit, test_dataloader)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ DEMO COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nResults saved to: {trainer.logger.log_dir}\")\n",
    "print(f\"Metrics CSV: {trainer.logger.log_dir}/metrics.csv\")\n",
    "print(\"\\nTest Results:\")\n",
    "for key, value in results[0].items():\n",
    "    print(f\"  {key:20s}: {value:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dd1057",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✅ Loading the pretrained phoneme recognition model\n",
    "2. ✅ Creating the Phoneme_GAT deepfake detection model\n",
    "3. ✅ Loading the ASVspoof 2019 LA dataset from HuggingFace\n",
    "4. ✅ Setting up PyTorch Lightning training\n",
    "5. ✅ Training and evaluating the model\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**For RunPod deployment:**\n",
    "1. Upload this notebook and all code to RunPod\n",
    "2. Run `bash setup_runpod.sh` to install dependencies\n",
    "3. Increase `NUM_EPOCHS` and `NUM_TRAIN_SAMPLES` for production training\n",
    "\n",
    "**For local use:**\n",
    "- Metrics are saved in the logs directory\n",
    "- View training progress: `cat logs/lightning_logs/version_X/metrics.csv`\n",
    "- Best checkpoint is saved automatically\n",
    "\n",
    "**Configuration for full training:**\n",
    "```python\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 16  # Adjust based on GPU memory\n",
    "NUM_TRAIN_SAMPLES = len(dataset['train'])  # Use full dataset\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
